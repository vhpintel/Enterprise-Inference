# Copyright (C) 2025-2026 Intel Corporation
# SPDX-License-Identifier: Apache-2.0
---
- name: Deploy/Remove Inference LLM Models
  hosts: "{{ inference_delegate | default('kube_control_plane') }}"
  gather_facts: false
  any_errors_fatal: "{{ any_errors_fatal | default(true) }}"
  environment: "{{ proxy_disable_env | default(env_proxy | default({})) }}"
  vars_files:
    - "{{ lookup('env', 'PWD') }}/config/vault.yml"
    - "{{ lookup('env', 'PWD') }}/config/vars/inference_llm_models.yml"
    - "{{ lookup('env', 'PWD') }}/config/inference_env.yml"
  roles:
    - role: inference-tools
  tasks:
    - name: Print active tags for this task
      debug:
        var: ansible_run_tags
      tags: always
      run_once: true

    - name: Display deployment configuration
      debug:
        msg:
          - "==================================================================="
          - "Inference Model Deployment Configuration"
          - "==================================================================="
          - "CPU Deployment: {{ cpu_playbook }}"
          - "GPU Deployment: {{ gpu_playbook }}"
          - "NRI CPU Balloons: {{ 'ENABLED' if cpu_playbook == 'true' else 'DISABLED' }}"
          - "Models to Deploy: {{ model_name_list }}"
      tags: always
      run_once: true
      when: balloon_policy_cpu == "enabled"

    - name: Set default model parallelism configuration for CPU deployments
      ansible.builtin.set_fact:
        model_parallelism_config:
          strategy: "{{ model_parallelism_config.strategy | default('tensor_parallel') }}"
      when:
        - cpu_playbook == 'true'
        - model_parallelism_config is not defined
      tags: always
      run_once: true

    - name: Setup Environment
      block:
        - name: Create/Update Kubernetes Secret for Hugging Face Token
          kubernetes.core.k8s:
            name: hugging-face-token
            namespace: default
            kind: Secret
            definition:
              stringData:
                hugging-face-token: "{{ hugging_face_token }}"
            apply: yes
            state: present
          run_once: true
          when: hugging_face_token is defined
          tags: always
        - name: Ensure Remote Directory Exists
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}"
            state: directory
            mode: "0755"
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"
          tags: always
        - name: Sync dependency files to Deployment Nodes
          ansible.posix.synchronize:
            src: "{{ item.src }}/"
            dest: "{{ item.dest }}/"
            recursive: yes
            delete: no
            mode: push
          loop:
            - {
                src: "{{ helm_charts_base }}",
                dest: "{{ remote_helm_charts_base }}/",
              }
          run_once: true
          tags: always
        - name: Use EKS-specific ingress configuration for VLLM
          ansible.builtin.copy:
            src: "{{ remote_helm_charts_base }}/vllm/templates/ingress_eks.yaml"
            dest: "{{ remote_helm_charts_base }}/vllm/templates/ingress.yaml"
            mode: "0644"
            remote_src: yes
          when: kubernetes_platform == "eks"
          run_once: true
          tags: always
        - name: Remove duplicate EKS ingress template for VLLM
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/vllm/templates/ingress_eks.yaml"
            state: absent
          when: kubernetes_platform == "eks"
          run_once: true
          tags: always
        - name: Remove EKS ingress template for VLLM when not on EKS
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/vllm/templates/ingress_eks.yaml"
            state: absent
          when: kubernetes_platform != "eks"
          run_once: true
          tags: always
        - name: Use EKS-specific ingress configuration for TEI
          ansible.builtin.copy:
            src: "{{ remote_helm_charts_base }}/tei/templates/ingress_eks.yaml"
            dest: "{{ remote_helm_charts_base }}/tei/templates/ingress.yaml"
            mode: "0644"
            remote_src: yes
          when: kubernetes_platform == "eks"
          run_once: true
          tags: always
        - name: Remove duplicate EKS ingress template for TEI
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/tei/templates/ingress_eks.yaml"
            state: absent
          when: kubernetes_platform == "eks"
          run_once: true
          tags: always
        - name: Remove EKS ingress template for TEI when not on EKS
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/tei/templates/ingress_eks.yaml"
            state: absent
          when: kubernetes_platform != "eks"
          run_once: true
          tags: always
        - name: Use EKS-specific ingress configuration for TEI Rerank
          ansible.builtin.copy:
            src: "{{ remote_helm_charts_base }}/teirerank/templates/ingress_eks.yaml"
            dest: "{{ remote_helm_charts_base }}/teirerank/templates/ingress.yaml"
            mode: "0644"
            remote_src: yes
          when: kubernetes_platform == "eks"
          run_once: true
          tags: always
        - name: Remove duplicate EKS ingress template for TEI Rerank
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/teirerank/templates/ingress_eks.yaml"
            state: absent
          when: kubernetes_platform == "eks"
          run_once: true
          tags: always
        - name: Remove EKS ingress template for TEI Rerank when not on EKS
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/teirerank/templates/ingress_eks.yaml"
            state: absent
          when: kubernetes_platform != "eks"
          run_once: true
          tags: always
        - name: Use EKS-specific ingress configuration for TGI
          ansible.builtin.copy:
            src: "{{ remote_helm_charts_base }}/tgi/templates/ingress_eks.yaml"
            dest: "{{ remote_helm_charts_base }}/tgi/templates/ingress.yaml"
            mode: "0644"
            remote_src: yes
          when: kubernetes_platform == "eks"
          run_once: true
          tags: always
        - name: Remove duplicate EKS ingress template for TGI
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/tgi/templates/ingress_eks.yaml"
            state: absent
          when: kubernetes_platform == "eks"
          run_once: true
          tags: always
        - name: Remove EKS ingress template for TGI when not on EKS
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/tgi/templates/ingress_eks.yaml"
            state: absent
          when: kubernetes_platform != "eks"
          run_once: true
          tags: always

        - name: Verify only one ingress template exists for each chart (EKS)
          ansible.builtin.shell: |
            echo "VLLM templates:"
            ls -la {{ remote_helm_charts_base }}/vllm/templates/ingress*.yaml || echo "No files found"
            echo "TEI templates:"
            ls -la {{ remote_helm_charts_base }}/tei/templates/ingress*.yaml || echo "No files found"
            echo "TEI Rerank templates:"
            ls -la {{ remote_helm_charts_base }}/teirerank/templates/ingress*.yaml || echo "No files found"
            echo "TGI templates:"
            ls -la {{ remote_helm_charts_base }}/tgi/templates/ingress*.yaml || echo "No files found"
          register: template_verification
          when: kubernetes_platform == "eks"
          run_once: true
          tags: always

        # OpenShift-specific tasks: Remove all ingress templates and keep only route templates
        - name: Remove standard ingress template for VLLM (OpenShift)
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/vllm/templates/ingress.yaml"
            state: absent
          when: kubernetes_platform == "openshift"
          run_once: true
          tags: always
        - name: Remove EKS ingress template for VLLM (OpenShift)
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/vllm/templates/ingress_eks.yaml"
            state: absent
          when: kubernetes_platform == "openshift"
          run_once: true
          tags: always
        - name: Remove standard ingress template for TEI (OpenShift)
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/tei/templates/ingress.yaml"
            state: absent
          when: kubernetes_platform == "openshift"
          run_once: true
          tags: always
        - name: Remove EKS ingress template for TEI (OpenShift)
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/tei/templates/ingress_eks.yaml"
            state: absent
          when: kubernetes_platform == "openshift"
          run_once: true
          tags: always
        - name: Remove standard ingress template for TEI Rerank (OpenShift)
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/teirerank/templates/ingress.yaml"
            state: absent
          when: kubernetes_platform == "openshift"
          run_once: true
          tags: always
        - name: Remove EKS ingress template for TEI Rerank (OpenShift)
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/teirerank/templates/ingress_eks.yaml"
            state: absent
          when: kubernetes_platform == "openshift"
          run_once: true
          tags: always
        - name: Remove standard ingress template for TGI (OpenShift)
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/tgi/templates/ingress.yaml"
            state: absent
          when: kubernetes_platform == "openshift"
          run_once: true
          tags: always
        - name: Remove EKS ingress template for TGI (OpenShift)
          ansible.builtin.file:
            path: "{{ remote_helm_charts_base }}/tgi/templates/ingress_eks.yaml"
            state: absent
          when: kubernetes_platform == "openshift"
          run_once: true
          tags: always

        - name: Display template verification results
          debug:
            msg: "{{ template_verification.stdout_lines }}"
          when:
            - kubernetes_platform == "eks"
            - template_verification is defined
          run_once: true
          tags: always
    - name: CPU Topology Detection and Optimization (for CPU deployments only)
      when: cpu_playbook == 'true' and balloon_policy_cpu == "enabled"
      run_once: true
      tags: always
      block:
        - name: Perform CPU topology detection and optimization
          ansible.builtin.include_role:
            name: utils
            tasks_from: get_optimized_cpu_topology
          vars:
            target_model: "llama-8b" # Default model for topology calculation
        - name: Calculate CPU allocation for model deployment
          ansible.builtin.set_fact:
            calculated_topology:
              total_sockets: "{{ cpu_topology_config.total_sockets | int }}"
              numa_nodes_per_socket: "{{ cpu_topology_config.numa_nodes_per_socket | int }}"
              cpus_per_socket: "{{ cpu_topology_config.cpus_per_socket | int }}"
              is_single_node: "{{ cpu_topology_config.total_sockets == 1 and cpu_topology_config.numa_nodes_per_socket == 1 }}"
              reserved_cpus_percentage: "{{ reserved_cpus_percentage | default(18) | int }}"
              reserved_cpus: >-
                {%- set reserved_cpu_count = nri_reserved_cpu_list.split(',') | length -%}
                {{ reserved_cpu_count }}
              balloon_cpus: >-
                {%- set total_cpus = (cpu_topology_config.cpus_per_socket | int) * (cpu_topology_config.total_sockets | int) -%}
                {%- set reserved_cpu_count = nri_reserved_cpu_list.split(',') | length -%}
                {%- if reserved_cpu_count >= total_cpus -%}{%- set reserved_cpu_count = total_cpus -%}{%- endif -%}
                {{ total_cpus - reserved_cpu_count }}
              workload_cpus: "{{ cpu_topology_config.workload_cpus | int }}"

        - name: Calculate parallelism configuration
          ansible.builtin.set_fact:
            parallelism_config:
              tensor_parallel_size: >-
                {%- set total_numa = (cpu_topology_config.total_sockets | int) * (cpu_topology_config.numa_nodes_per_socket | int) -%}
                {%- if total_numa in [1, 2, 4] -%}
                {{ total_numa }}
                {%- elif total_numa in [3] -%}
                2
                {%- elif total_numa in [6] -%}
                4
                {%- else -%}
                1
                {%- endif -%}
              strategy: >-
                tensor_parallel

        - name: Combine topology and parallelism configs
          ansible.builtin.set_fact:
            optimal_balloon_config: "{{ calculated_topology | combine(parallelism_config) }}"

        - name: Display detected CPU topology for model deployment
          debug:
            msg:
              - "==================================================================="
              - "CPU Topology Detection Results for Model Deployment"
              - "==================================================================="
              - "Target Worker Node: {{ optimal_balloon_config.target_worker_node | default('auto-detected') }}"
              - "Total Sockets: {{ optimal_balloon_config.total_sockets }}"
              - "NUMA Nodes per Socket: {{ optimal_balloon_config.numa_nodes_per_socket }}"
              - "CPUs per Socket: {{ optimal_balloon_config.cpus_per_socket }}"
              - "Is Single Node: {{ optimal_balloon_config.is_single_node }}"
              - "Reserved CPUs Count: {{ optimal_balloon_config.reserved_cpus }} ({{ ((optimal_balloon_config.reserved_cpus | int / optimal_balloon_config.cpus_per_socket | int) * 100) | round(1) }}% actual)"
              - "Workload CPUs (for xeon-values.yaml): {{ optimal_balloon_config.workload_cpus }}"
              - "Balloon CPUs: {{ optimal_balloon_config.balloon_cpus }}"
              - "Tensor Parallel Size: {{ optimal_balloon_config.tensor_parallel_size | default(1) }}"
              - "Pipeline Parallel Size: {{ optimal_balloon_config.pipeline_parallel_size | default(1) }}"
              - "Strategy: {{ optimal_balloon_config.strategy }}"
              - "==================================================================="

        - name: Check if vLLM helm charts directory exists
          ansible.builtin.stat:
            path: "{{ remote_helm_charts_base }}/vllm"
          register: vllm_charts_dir

        - name: Display helm charts path status
          debug:
            msg:
              - "Helm charts path: {{ remote_helm_charts_base }}/vllm"
              - "Directory exists: {{ vllm_charts_dir.stat.exists }}"

        - name: Warning when vLLM helm charts not found
          debug:
            msg:
              - "==================================================================="
              - "WARNING: vLLM Helm Charts Directory Not Found"
              - "==================================================================="
              - "Expected path: {{ remote_helm_charts_base }}/vllm/xeon-values.yaml"
              - "This means vLLM chart values cannot be automatically updated"
              - "Model deployment will continue but may not be optimized"
              - "==================================================================="
          when: not vllm_charts_dir.stat.exists

        - name: Extract sockets memory
          ansible.builtin.set_fact:
            usable_mem_var: "{{ cpu_topology_config.usable_memory_per_socket }}"
            socket0_memory_gb: 0

        - name: Extract socket0 memory
          ansible.builtin.set_fact:
            socket0_memory_gb: "{{ usable_mem_var[0] | int }}"
          when: usable_mem_var is mapping and socket0_memory_gb | int == 0 and 0 in usable_mem_var
          ignore_errors: true

        - name: Fallback to parsing from string if dictionary access failed
          ansible.builtin.set_fact:
            socket0_memory_gb: "{{ usable_mem_var | string | regex_search('0: (\\d+)') | default(['0', '0']) | last | int }}"
          when: usable_mem_var is string and socket0_memory_gb | int == 0
          ignore_errors: true

        - name: Calculate minimum memory based on tensor parallel size
          ansible.builtin.set_fact:
            min_memory_for_tp: >-
              {%- set tp = optimal_balloon_config.tensor_parallel_size | default(1) | int -%}
              {%- if tp >= 4 -%}
              256
              {%- elif tp >= 2 -%}
              128
              {%- else -%}
              128
              {%- endif -%}

        - name: Set optimal memory based on TP requirements and available memory
          ansible.builtin.set_fact:
            optimal_memory_gb: >-
              {%- set available = socket0_memory_gb | int -%}
              {%- set min_required = min_memory_for_tp | int -%}
              {%- set max_limit = 256 -%}
              {%- if available < min_required -%}
              {{ min_required }}
              {%- elif available > max_limit -%}
              {{ max_limit }}
              {%- else -%}
              {{ available }}
              {%- endif -%}

        - name: Final memory calculation
          ansible.builtin.debug:
            msg: |
              === Memory Calculation Summary ===
              Tensor Parallel Size: {{ optimal_balloon_config.tensor_parallel_size | default(1) }}
              Minimum memory required for TP: {{ min_memory_for_tp }} GB
              Socket 0 memory available: {{ socket0_memory_gb }} GB
              Final memory for deployment: {{ optimal_memory_gb }} GB
              Memory allocation: {{ 'Using minimum required' if (socket0_memory_gb | int) < (min_memory_for_tp | int) else 'Using available memory' if (socket0_memory_gb | int) <= 256 else 'Capped at 256GB max' }}

        - name: Apply optimized TP/PP defaults (unless user overrides)
          ansible.builtin.set_fact:
            tensor_parallel_size: "{{ tensor_parallel_size |  default(optimal_balloon_config.get('tensor_parallel_size', 1)) | int }}"
            pipeline_parallel_size: "{{ pipeline_parallel_size | default(optimal_balloon_config.get('pipeline_parallel_size', 1)) | int }}"
          when:
            - cpu_playbook == 'true'
          tags: always
          run_once: true

        - name: Display CPU optimization helm charts update completion
          debug:
            msg:
              - "==================================================================="
              - "CPU Optimization Configuration Complete"
              - "==================================================================="
              - "✓ CPU topology detected and analyzed"
              - "✓ Memory calculated ({{ optimal_memory_gb | default('8') }}Gi based on available socket memory)"
              - "✓ Optimized values prepared for helm deployment"
              - "✓ CPU: {{ optimal_balloon_config.workload_cpus | default(8) }} cores"
              - "✓ Memory: {{ optimal_memory_gb | default('8') }}Gi"
              - "✓ Tensor Parallel Size: {{ optimal_balloon_config.tensor_parallel_size | default(1) }}"
              - "✓ Pipeline Parallel Size: {{ optimal_balloon_config.pipeline_parallel_size | default(1) }}"
              - "✓ Values will be set via --set flags during helm install"
              - "✓ xeon-values.yaml remains unchanged (template-based)"
              - "✓ Ready for model deployment with NRI balloon policy"
              - "==================================================================="

    - name: Setup Environment (continued)
      block:
        - name: Transfer Dependency keycloak-realmcreationfile
          ansible.builtin.copy:
            src: "{{ remote_home_dir }}/"
            dest: "{{ remote_helm_charts_base }}/"
            mode: "0755"
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"
            remote_src: no
          tags: install-keycloak-apisix
        - name: Ensure the script is executable
          file:
            path: "{{ remote_helm_charts_base }}/keycloak-realmcreation.sh"
            mode: "0755"
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"
          tags: install-keycloak-apisix
        - name: Update Dependencies for Auth-Apisix on Nodes
          ansible.builtin.command: helm dependency update "{{ remote_helm_charts_base }}/apisix-helm/"
          register: helm_dependency_update
          failed_when: helm_dependency_update.rc != 0
          when: apisix_enabled == "yes"
          tags: install-keycloak-apisix
    - name: Fetch the keycloak client secret
      block:
        - name: Create ConfigMap with Keycloak realm creation script
          kubernetes.core.k8s:
            state: present
            definition:
              apiVersion: v1
              kind: ConfigMap
              metadata:
                name: keycloak-realm-script
                namespace: default
              data:
                keycloak-realmcreation.sh: "{{ lookup('file', remote_home_dir + '/keycloak-realmcreation.sh') }}"
          run_once: true

        - name: Create Keycloak realm setup Job
          kubernetes.core.k8s:
            state: present
            definition:
              apiVersion: batch/v1
              kind: Job
              metadata:
                name: keycloak-realm-setup-models
                namespace: default
              spec:
                ttlSecondsAfterFinished: 300
                backoffLimit: 3
                template:
                  metadata:
                    name: keycloak-realm-setup
                  spec:
                    restartPolicy: Never
                    containers:
                    - name: keycloak-setup
                      image: ubuntu:22.04
                      command: ["/bin/bash", "-c"]
                      args:
                        - |
                          set -e

                          # Install dependencies
                          export DEBIAN_FRONTEND=noninteractive
                          apt-get update -qq
                          apt-get install -y -qq curl jq

                          # Copy script to writable location
                          cp /scripts/keycloak-realmcreation.sh /tmp/keycloak-realmcreation.sh
                          chmod +x /tmp/keycloak-realmcreation.sh

                          # Wait for Keycloak to be ready with retry logic
                          MAX_RETRIES=30
                          RETRY_DELAY=10
                          RETRY_COUNT=0

                          echo "Waiting for Keycloak to be ready..."
                          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
                            echo "Attempt $((RETRY_COUNT + 1))/$MAX_RETRIES: Testing Keycloak authentication..."

                            HTTP_CODE=$(curl -s -o /dev/null -w "%{http_code}" \
                              -X POST \
                              -d "client_id=admin-cli" \
                              -d "username={{ keycloak_admin_user }}" \
                              -d "password={{ keycloak_admin_password }}" \
                              -d "grant_type=password" \
                              http://keycloak.default.svc.cluster.local:80/realms/master/protocol/openid-connect/token)

                            if [ "$HTTP_CODE" = "200" ]; then
                              echo "Keycloak is ready and authenticated successfully"
                              break
                            else
                              echo "Keycloak not ready yet (HTTP $HTTP_CODE), waiting ${RETRY_DELAY}s..."
                              RETRY_COUNT=$((RETRY_COUNT + 1))
                              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                                sleep $RETRY_DELAY
                              fi
                            fi
                          done

                          if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
                            echo "ERROR: Keycloak did not become ready after $MAX_RETRIES attempts"
                            exit 1
                          fi

                          # Additional stabilization wait
                          echo "Waiting 15 seconds for Keycloak to stabilize..."
                          sleep 15

                          # Run the realm creation script
                          echo "Running Keycloak realm creation script..."
                          /tmp/keycloak-realmcreation.sh \
                            keycloak.default.svc.cluster.local:80 \
                            "{{ keycloak_admin_user }}" \
                            "{{ keycloak_admin_password }}" \
                            "{{ keycloak_client_id }}"
                      volumeMounts:
                      - name: scripts
                        mountPath: /scripts
                        readOnly: true
                    volumes:
                    - name: scripts
                      configMap:
                        name: keycloak-realm-script
                        defaultMode: 0755
                    tolerations:
                      - key: node-role.kubernetes.io/control-plane
                        operator: Exists
                        effect: NoSchedule
                      - key: node-role.kubernetes.io/master
                        operator: Exists
                        effect: NoSchedule
          run_once: true

        - name: Wait for Keycloak realm setup Job to complete
          kubernetes.core.k8s_info:
            kind: Job
            namespace: default
            name: keycloak-realm-setup-models
          register: realm_job
          until: >
            realm_job.resources[0].status.succeeded is defined and
            realm_job.resources[0].status.succeeded == 1
          retries: 30
          delay: 10
          run_once: true

        - name: Get Keycloak realm setup Job logs
          command: kubectl logs job/keycloak-realm-setup-models -n default
          register: script_output
          run_once: true

        - name: Set Keycloak client fact
          set_fact:
            client_secret: "{{ script_output.stdout | regex_search('Client secret: (.*)') | join('') | regex_replace('^Client secret: ') }}"
          when: script_output.stdout is search('Client secret:')
          run_once: true

        - name: Warning when client secret not found
          debug:
            msg: "WARNING: Client secret was not found in the Keycloak setup job output. Please check the job logs manually."
          when: client_secret is not defined
          run_once: true
      run_once: true
      when: deploy_keycloak == 'yes'
      tags: install-keycloak-apisix
    - name: env_proxy Contents
      debug:
        var: env_proxy
      tags: always
    - name: Set proxy args if proxy is defined
      set_fact:
        helm_proxy_args: >-
          {% if env_proxy is defined %}
          --set global.http_proxy="{{ env_proxy.http_proxy | default('') | replace(',', '\\,') }}"
          --set global.https_proxy="{{ env_proxy.https_proxy | default('') | replace(',', '\\,') }}"
          --set global.no_proxy="{{ env_proxy.no_proxy | default('') | replace(',', '\\,') }}"
          {% else %}
          {{ '' }}
          {% endif %}
      tags: always
    - name: Print model_name_list
      debug:
        var: model_name_list
      tags: always
      run_once: true
    - name: Deploy Llama8b LLM Model
      block:
        - name: Delete Ingress resource Llama8b from default namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-llama-8b-ingress
            state: absent
          tags: install-llama-8b
        - name: Delete Ingress resource Llama8b from auth-apisix namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-llama-8b-ingress
            state: absent
          tags: install-llama-8b
        - name: Deploy LLM model Llama8b
          ansible.builtin.command: >-
            helm upgrade --install vllm-llama-8b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="meta-llama/Llama-3.1-8B-Instruct"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if gaudi_deployment %}
              --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
            {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
          tags: install-llama-8b
        - name: Register Llama8b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "meta-llama/Llama-3.1-8B-Instruct"
            reg_litellm_model: "openai/meta-llama/Llama-3.1-8B-Instruct"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-llama-8b-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-llama-8b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-llama-8b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      when:
        - model_name_list is defined
        - "'llama-8b' in (model_name_list | regex_replace(',', ' ') | split())"
        - install_true == 'true'

    - name: Check if vllm-llama-8b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-llama-8b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-llama-8b
    - name: Uninstall vllm-llama-8b Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-llama-8b"
      run_once: true
      tags: uninstall-llama-8b
      when:
        - "'llama-8b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy "{{ huggingface_model_id }}" LLM Model
      block:
        - name: Delete Ingress resource "{{ huggingface_model_id }}" from default namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: "{{huggingface_model_deployment_name}}-ingress"
            state: absent
          tags: "install-{{ huggingface_model_deployment_name }}"
        - name: Delete Ingress resource "{{ huggingface_model_id }}" from auth-apisix namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: "{{huggingface_model_deployment_name}}-ingress"
            state: absent
          tags: "install-{{ huggingface_model_deployment_name }}"
        - name: Deploy LLM model "{{ huggingface_model_id }}"
          ansible.builtin.command: >-
            helm upgrade --install "{{huggingface_model_deployment_name}}" "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="{{ huggingface_model_id }}"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if cpu_playbook == 'true' %}
              --values {{ remote_helm_charts_base }}/vllm/xeon-values.yaml
              --set cpu_balloon_annotation="vllm-balloon"
              --set podLabels.name="vllm"
              --set cpu="{{ optimal_balloon_config.workload_cpus | default(8) }}"
              --set memory="{{ optimal_memory_gb | default(8) }}Gi"
              --set tensor_parallel_size={{ optimal_balloon_config.tensor_parallel_size | default(1) }}
              --set pipeline_parallel_size={{ optimal_balloon_config.pipeline_parallel_size | default(1) }}
            {% elif gaudi_deployment|lower == "true" %}
              --set tensor_parallel_size={{ huggingface_tensor_parellel_size }}
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
            {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register "{{ huggingface_model_id }}" model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "{{ huggingface_model_id }}"
            reg_litellm_model: "openai/{{ huggingface_model_id }}"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://{{huggingface_model_deployment_name}}-vllm-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - "install-{{ huggingface_model_deployment_name }}"
            - install-genai-gateway
          run_once: true
          when:
            - "'install-{{ huggingface_model_deployment_name }}' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: "install-{{ huggingface_model_deployment_name }}"
      when:
        - hugging_face_model_deployment == 'true'
        - install_true == 'true'
    - name: Check if "{{ hugging_face_model_remove_name }}" Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter {{ hugging_face_model_remove_name }} --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: "uninstall-{{ hugging_face_model_remove_name }}"
    - name: Uninstall "{{ hugging_face_model_remove_name }}" Model
      ansible.builtin.command:
        cmd: "helm uninstall {{ hugging_face_model_remove_name }}"
      run_once: true
      tags: "uninstall-{{ hugging_face_model_remove_name }}"
      when:
        - uninstall_true == 'true'
        - hugging_face_model_remove_deployment == 'true'
        - helm_release_installed.stdout != ""



    - name: Check if Llama4 Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-llama-4-scout-17b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-llama-4-scout-17b
    - name: Uninstall vllm-llama-8b Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-llama-4-scout-17b"
      run_once: true
      tags: uninstall-llama-4-scout-17b
      when:
        - "'llama-4-scout-17b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""


    - name: Deploy Llama4-17B Scout LLM Model
      block:
        - name: Delete Ingress resource Llama4-17B from default namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-llama-4-scout-17b-ingress
            state: absent
          tags: install-llama-4-scout-17b
        - name: Delete Ingress resource Llama4-17B from auth-apisix namespace
          tags: install-llama-4-scout-17b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-llama-4-scout-17b-ingress
            state: absent
        - name: Deploy LLM model Llama 4 17B
          ansible.builtin.command: >-
            helm upgrade --install vllm-llama-4-scout-17b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="meta-llama/Llama-4-Scout-17B-16E-Instruct"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            --set tensor_parallel_size=4
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if gaudi_deployment %}
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
            {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register llama-4-scout-17b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "meta-llama/Llama-4-Scout-17B-16E-Instruct"
            reg_litellm_model: "openai/meta-llama/Llama-4-Scout-17B-16E-Instruct"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-llama-4-scout-17b-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-llama-4-scout-17b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-llama-4-scout-17b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-llama-4-scout-17b
      when:
        - '"llama-4-scout-17b" in model_name_list'
        - install_true == 'true'



    - name: Check if QWEN 2.5 3B Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-qwen-2-5-32b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-qwen-2-5-32b
    - name: Uninstall QWEN 2.5 3B Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-qwen-2-5-32b"
      run_once: true
      tags: uninstall-qwen-2-5-32b
      when:
        - "'qwen-2-5-32b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy Qwen-2.5-32B Scout LLM Model
      block:
        - name: Delete Ingress resource Qwen-2.5-32B from default namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-qwen-2-5-32b-ingress
            state: absent
          tags: install-qwen-2-5-32b
        - name: Delete Ingress resource Qwen-2-5-32B from auth-apisix namespace
          tags: install-qwen-2-5-32b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-qwen-2-5-32b-ingress
            state: absent
        - name: Deploy LLM model Qwen-2-5-32B
          ansible.builtin.command: >-
            helm upgrade --install vllm-qwen-2-5-32b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="Qwen/Qwen2.5-32B-Instruct"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            --set tensor_parallel_size=1
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if gaudi_deployment %}
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
            {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Qwen2.5-32B model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "Qwen/Qwen2.5-32B-Instruct"
            reg_litellm_model: "openai/Qwen/Qwen2.5-32B-Instruct"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-qwen-2-5-32b-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-qwen-2-5-32b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-qwen-2-5-32b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-qwen-2-5-32b
      when:
        - '"qwen-2-5-32b" in model_name_list'
        - install_true == 'true'

    - name: Check if vllm-llama-70b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-llama-70b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-llama-70b
    - name: Uninstall llama-70b Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-llama-70b"
      run_once: true
      tags: uninstall-llama-70b
      when:
        - "'llama-70b' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""





    - name: Deploy Llama70b LLM Model
      block:
        - name: Delete Ingress resource Llama70b from default namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-llama-70b-ingress
            state: absent
          tags: install-llama-70b
        - name: Delete Ingress resource Llama70b from auth-apisix namespace
          tags: install-llama-70b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-llama-70b-ingress
            state: absent
        - name: Deploy LLM model Llama70b
          ansible.builtin.command: >-
            helm upgrade --install vllm-llama-70b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="meta-llama/Llama-3.1-70B-Instruct"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            --set tensor_parallel_size=4
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if gaudi_deployment %}
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
            {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Llama70b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "meta-llama/Llama-3.1-70B-Instruct"
            reg_litellm_model: "openai/meta-llama/Llama-3.1-70B-Instruct"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-llama-70b-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-llama-70b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-llama-70b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-llama-70b
      when:
        - '"llama-70b" in model_name_list'
        - install_true == 'true'

    - name: Check if vllm-llama-70b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-llama-70b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-llama-70b
    - name: Uninstall llama-70b Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-llama-70b"
      run_once: true
      tags: uninstall-llama-70b
      when:
        - "'llama-70b' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy Llama3-3-70b LLM Model
      block:
        - name: Delete Ingress resource Llama3.3-70b from default namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-llama-3-3-70b-ingress
            state: absent
          tags: install-llama-3-3-70b
        - name: Delete Ingress resource Llama3-3-70b from auth-apisix namespace
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-llama-3-3-70b-ingress
            state: absent
          tags: install-llama-3-3-70b
        - name: Deploy LLM model Llama3-3-70b
          ansible.builtin.command: >-
            helm upgrade --install vllm-llama-3-3-70b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="meta-llama/Llama-3.3-70B-Instruct"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if gaudi_deployment %}
              --set tensor_parallel_size=4
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
            {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
          tags: install-llama-3-3-70b
        - name: Register LLM model Llama3-3-70b
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "meta-llama/Llama-3.3-70B-Instruct"
            reg_litellm_model: "openai/meta-llama/Llama-3.3-70B-Instruct"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-llama-3-3-70b-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-llama-3-3-70b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-llama-3-3-70b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      when:
        - model_name_list is defined
        - "'llama-3-3-70b' in (model_name_list | regex_replace(',', ' ') | split())"
        - install_true == 'true'

    - name: Check if vllm-llama-3-3-70b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-llama-3-3-70b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-llama-3-3-70b
    - name: Uninstall vllm-llama-3-3-70b Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-llama-3-3-70b"
      run_once: true
      tags: uninstall-llama-3-3-70b
      when:
        - "'llama-3-3-70b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy Mixtral7b LLM Model
      block:
        - name: Delete Ingress resource Mixtral7b from default namespace
          tags: install-mixtral-8x-7b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-mixtral-7b-ingress
            state: absent
        - name: Delete Ingress resource Mixtral7b from auth-apisix namespace
          tags: install-mixtral-8x-7b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-mixtral-7b-ingress
            state: absent
        - name: Deploy LLM model Mixtral7b
          ansible.builtin.command: >-
            helm upgrade --install vllm-mixtral-7b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="mistralai/Mixtral-8x7B-Instruct-v0.1"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if gaudi_deployment %}
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
            {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Mixtral7b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "mistralai/Mixtral-8x7B-Instruct-v0.1"
            reg_litellm_model: "openai/mistralai/Mixtral-8x7B-Instruct-v0.1"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-mixtral-7b-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-mixtral-8x-7b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-mixtral-8x-7b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-mixtral-8x-7b
      when:
        - '"mixtral-8x-7b" in model_name_list'
        - install_true == 'true'

    - name: Check if Mixtral7b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-mixtral-7b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-mixtral-8x-7b
    - name: Uninstall mixtral-8x-7b Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-mixtral-7b"
      run_once: true
      tags: uninstall-mixtral-8x-7b
      when:
        - "'mixtral-8x-7b' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy codellama LLM Model
      block:
        - name: Delete Ingress resource Codellama from default namespace
          tags: install-codellama-34b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-codellama-ingress
            state: absent
        - name: Delete Ingress resource Codellama from auth-apisix namespace
          tags: install-codellama-34b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-codellama-ingress
            state: absent
        - name: Deploy LLM model codellama
          ansible.builtin.command: >-
            helm upgrade --install vllm-codellama "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="codellama/CodeLlama-34b-Instruct-hf"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if gaudi_deployment %}
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
            {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Codellama model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "codellama/CodeLlama-34b-Instruct-hf"
            reg_litellm_model: "openai/codellama/CodeLlama-34b-Instruct-hf"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-codellama-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-codellama-34b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-codellama-34b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      tags: install-codellama-34b
      when:
        - '"codellama-34b" in model_name_list'
        - install_true == 'true'
    - name: Check if codellama-34b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-codellama --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-codellama-34b
    - name: Uninstall codellama-34b Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-codellama"
      run_once: true
      tags: uninstall-codellama-34b
      when:
        - "'codellama-34b' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy Faclon3 7b LLM Model
      block:
        - name: Delete Ingress resource Falcon3 7b from default namespace
          tags: install-falcon3-7b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-falcon3-7b-ingress
            state: absent
        - name: Delete Ingress resource Falcon3 7b from auth-apisix namespace
          tags: install-falcon3-7b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-falcon3-7b-ingress
            state: absent
        - name: Deploy LLM model Falcon3-7b
          ansible.builtin.command: >-
            helm upgrade --install vllm-falcon3-7b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="tiiuae/Falcon3-7B-Instruct"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if gaudi_deployment %}
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
            {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Falcon3-7b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "tiiuae/Falcon3-7B-Instruct"
            reg_litellm_model: "openai/tiiuae/Falcon3-7B-Instruct"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-falcon3-7b-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-falcon3-7b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-falcon3-7b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-falcon3-7b
      when:
        - '"falcon3-7b" in model_name_list'
        - install_true == 'true'
    - name: Check if Falcon3 7b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-falcon3-7b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-falcon3-7b
    - name: Uninstall Falcon3 7b Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-falcon3-7b"
      run_once: true
      tags: uninstall-falcon3-7b
      when:
        - "'falcon3-7b' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy Mistral_7b LLM Model
      block:
        - name: Delete Ingress resource Mistral 7b from default namespace
          tags: install-mistral-7b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-mistral-7b-ingress
            state: absent
        - name: Delete Ingress resource Mistral 7b from auth-apisix namespace
          tags: install-mistral-7b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-mistral-7b-ingress
            state: absent
        - name: Deploy LLM model Mistral_7b
          ansible.builtin.command: >-
            helm upgrade --install vllm-mistral-7b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="mistralai/Mistral-7B-Instruct-v0.3"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if gaudi_deployment %}
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
            {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Mistral_7b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "mistralai/Mistral-7B-Instruct-v0.3"
            reg_litellm_model: "openai/mistralai/Mistral-7B-Instruct-v0.3"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-mistral-7b-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-mistral-7b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-mistral-7b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-mistral-7b
      when:
        - '"mistral-7b" in model_name_list'
        - install_true == 'true'
    - name: Check if Mistral_7b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-mistral-7b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-mistral-7b
    - name: Uninstall Mistral_7b Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-mistral-7b"
      run_once: true
      tags: uninstall-mistral-7b
      when:
        - "'mistral-7b' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy tei LLM Model
      block:
        - name: Delete existing Ingress resource tei in auth-apisix namespace
          tags: install-tei
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-tei-ingress
            state: absent
        - name: Delete existing Ingress resource tei in default namespace
          tags: install-tei
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-tei-ingress
            state: absent
        - name: Deploy LLM model tei
          ansible.builtin.command: >-
            helm upgrade --install vllm-tei "{{ remote_helm_charts_base }}/tei"
            --set EMBEDDING_MODEL_ID="BAAI/bge-base-en-v1.5"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if gaudi_deployment %}
              --values "{{ remote_helm_charts_base }}/tei/gaudi-values.yaml"
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
            {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            --set teirerank.namespace=default
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register vllm-tei model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "BAAI/bge-base-en-v1.5"
            reg_litellm_model: "huggingface/BAAI/bge-base-en-v1.5"
            reg_custom_llm_provider: "huggingface"
            reg_api_base: "http://vllm-tei-service.default"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
            reg_model_mode: "embedding"
          tags:
            - install-tei
            - install-genai-gateway
          run_once: true
          when:
            - "'install-tei' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-tei
      when:
        - "'tei' in model_name_list"
        - install_true == 'true'
    - name: Check if tei Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-tei --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-tei
    - name: Uninstall tei Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-tei"
      run_once: true
      tags: uninstall-tei
      when:
        - "'tei' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy teirerank LLM Model
      block:
        - name: Delete Ingress resource Teirerank from default namespace
          tags: install-rerank
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-teirerank-ingress
            state: absent
        - name: Delete Ingress resource Teirerank from auth-apisix namespace
          tags: install-rerank
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-teirerank-ingress
            state: absent
        - name: Deploy LLM model teirerank
          ansible.builtin.command: >-
            helm upgrade --install vllm-teirerank "{{ remote_helm_charts_base }}/teirerank"
            --set RERANK_MODEL_ID="BAAI/bge-reranker-base"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if gaudi_deployment %}
              --values "{{ remote_helm_charts_base }}/teirerank/gaudi-values.yaml"
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
            {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            --set teirerank.namespace=default
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register vllm-teirerank model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "BAAI/bge-reranker-base"
            reg_litellm_model: "huggingface/BAAI/bge-reranker-base"
            reg_custom_llm_provider: "huggingface"
            reg_api_base: "http://vllm-teirerank-service.default"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
            reg_model_mode: "rerank"
          tags:
            - install-rerank
            - install-genai-gateway
          run_once: true
          when:
            - "'install-rerank' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-rerank
      when:
        - "'rerank' in model_name_list"
        - install_true == 'true'
    - name: Check if teirerank Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-teirerank --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-rerank
    - name: Uninstall teirerank Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-teirerank"
      run_once: true
      tags: uninstall-rerank
      when:
        - "'rerank' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy Deepkseek R1 Distill Qwen 32b LLM Model
      block:
        - name: Delete Ingress resource Deepkseek R1 Distill Qwen 32b from default namespace
          tags: install-deepseek-r1-distill-qwen-32b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-deepkseek-r1-qwen-32b-ingress
            state: absent
          run_once: true
        - name: Delete Ingress resource Deepkseek R1 Distill Qwen 32b from auth-apisix namespace
          tags: install-deepseek-r1-distill-qwen-32b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-deepkseek-r1-qwen-32b-ingress
            state: absent
          run_once: true
        - name: Deploy LLM model Deepkseek R1 Distill Qwen 32b
          ansible.builtin.command: >-
            helm upgrade --install vllm-deepkseek-r1-qwen-32b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if gaudi_deployment %}
              --set tensor_parallel_size=2
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
            {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Deepkseek R1 Distill Qwen 32b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            reg_litellm_model: "openai/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-deepkseek-r1-qwen-32b-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-deepseek-r1-distill-qwen-32b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-deepseek-r1-distill-qwen-32b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-deepseek-r1-distill-qwen-32b
      when:
        - "'deepseek-r1-distill-qwen-32b' in (model_name_list | regex_replace(',', ' ') | split())"
        - install_true == 'true'

    - name: Check if vllm-deepkseek-r1-qwen-32b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-deepkseek-r1-qwen-32b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-deepseek-r1-distill-qwen-32b
    - name: Uninstall vllm-deepkseek-r1-qwen-32b Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-deepkseek-r1-qwen-32b"
      run_once: true
      tags: uninstall-deepseek-r1-distill-qwen-32b
      when:
        - "'deepseek-r1-distill-qwen-32b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy Deepkseek R1 Distill Llama 8b LLM Model
      block:
        - name: Delete Ingress resource Deepkseek R1 Distill Llama 8b from default namespace
          tags: install-deepseek-r1-distill-llama8b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-deepkseek-r1-llama-8b-ingress
            state: absent
        - name: Delete Ingress resource Deepkseek R1 Distill Llama 8b from auth-apisix namespace
          tags: install-deepseek-r1-distill-llama8b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-deepkseek-r1-llama-8b-ingress
            state: absent
        - name: Deploy LLM model Deepkseek R1 Distill Llama 8b
          ansible.builtin.command: >-
            helm upgrade --install vllm-deepkseek-r1-llama-8b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if gaudi_deployment %}
              --set tensor_parallel_size={{ tensor_parallel_size_vllm }}
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
            {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Deepkseek R1 Distill Llama 8b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
            reg_litellm_model: "openai/deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-deepkseek-r1-llama-8b-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-deepseek-r1-distill-llama8b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-deepseek-r1-distill-llama8b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-deepseek-r1-distill-llama8b
      when:
        - "'deepseek-r1-distill-llama8b' in (model_name_list | regex_replace(',', ' ') | split())"
        - install_true == 'true'

    - name: Check if vllm-deepkseek-r1-llama-8b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-deepkseek-r1-llama-8b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-deepseek-r1-distill-llama8b
    - name: Uninstall vllm-deepkseek-r1-llama-8b Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-deepkseek-r1-llama-8b"
      run_once: true
      tags: uninstall-deepseek-r1-distill-llama8b
      when:
        - "'deepseek-r1-distill-llama8b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy Llama3.405b LLM Model
      block:
        - name: Delete Ingress resource Llama3.405b from default namespace
          tags: install-llama3-405b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-llama3-405b-ingress
            state: absent
        - name: Delete Ingress resource Llama3.405b from auth-apisix namespace
          tags: install-llama3-405b
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-llama3-405b-ingress
            state: absent
        - name: Deploy LLM model Llama3.405b
          ansible.builtin.command: >-
            helm upgrade --install vllm-llama3-405b "{{ remote_helm_charts_base }}/vllm"
            --set LLM_MODEL_ID="meta-llama/Llama-3.1-405B-Instruct"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if gaudi_deployment %}
              --set tensor_parallel_size=8
              --values {{ gaudi_values_file }}
            {% endif %}
            {% if ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
            {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment
          failed_when: helm_upgrade_install_model_deployment.rc != 0
        - name: Register Llama3.405b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "meta-llama/Llama-3.1-405B-Instruct"
            reg_litellm_model: "openai/meta-llama/Llama-3.1-405B-Instruct"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-llama3-405b-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-llama3-405b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-llama3-405b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-llama3-405b
      when:
        - "'llama3-405b' in model_name_list"
        - install_true == 'true'

    - name: Check if vllm-llama3-405b Model is deployed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-llama3-405b --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-llama3-405b
    - name: Uninstall Llama3.405b Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-llama3-405b"
      run_once: true
      tags: uninstall-llama3-405b
      when:
        - "'llama3-405b' in model_name_list"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Check if CPU Model is installed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-llama-8b-cpu --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-cpu-llama-8b
    - name: Uninstall CPU  Model
      ansible.builtin.command:
        cmd: "helm uninstall vllm-llama-8b-cpu"
      run_once: true
      tags: uninstall-cpu-llama-8b
      when:
        - model_name_list is defined
        - "'cpu-llama-8b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""


    - name: Deploy CPU based LLM model  llama 8b
      block:
        - name: Delete Ingress resource Llama8b from default namespace
          tags: install-cpu-llama-8b
          ignore_errors: yes
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-llama-8b-cpu-ingress
            state: absent
          ignore_errors: yes
        - name: Delete Ingress resource Llama8b from auth-apisix namespace
          tags: install-cpu-llama-8b
          ignore_errors: yes
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-llama-8b-cpu-ingress
            state: absent
          ignore_errors: yes
        - name: Deploy CPU based LLM model  llama 8b Installation
          ansible.builtin.command: >-
            helm upgrade --install vllm-llama-8b-cpu "{{ remote_helm_charts_base }}/vllm"
            --values "{{ xeon_values_file }}"
            --set LLM_MODEL_ID="meta-llama/Llama-3.1-8B-Instruct"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            {% if cpu_playbook == 'true' %}
            --set cpu_balloon_annotation="vllm-balloon"
            --set podLabels.name="vllm"
            --set cpu="{{ optimal_balloon_config.workload_cpus | default(8) }}"
            --set memory="{{ optimal_memory_gb | default(8) }}Gi"
            --set tensor_parallel_size={{ tensor_parallel_size | default(1) }}
            --set pipeline_parallel_size={{ pipeline_parallel_size | default(1) }}
            {% endif %}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if kubernetes_platform == 'openshift' %}
              --set route.enabled=true
              --set route.host={{ secret_name }}
              --set route.tls.termination=edge
              --set route.tls.insecureEdgeTerminationPolicy=Redirect
              --set ingress.enabled=false
            {% elif ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              --set route.enabled=false
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
              {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment_cpu_llama8b
          failed_when: helm_upgrade_install_model_deployment_cpu_llama8b.rc != 0
        - name: Register Xeon/CPU llama 8b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "meta-llama/Llama-3.1-8B-Instruct"
            reg_litellm_model: "openai/meta-llama/Llama-3.1-8B-Instruct"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-llama-8b-cpu-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-cpu-llama-8b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-cpu-llama-8b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-cpu-llama-8b
      when:
        - cpu_playbook == 'true'
        - install_true == 'true'
        - model_name_list is defined
        - "'cpu-llama-8b' in (model_name_list | regex_replace(',', ' ') | split())"

    - name: Check if CPU Deepseek R1 Distill Qwen 32b is installed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-deepseek-r1-qwen32b-cpu --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-cpu-deepseek-r1-distill-qwen-32b
    - name: Uninstall Deepseek R1 Distill Qwen 32b
      ansible.builtin.command:
        cmd: "helm uninstall vllm-deepseek-r1-qwen32b-cpu"
      run_once: true
      tags: uninstall-cpu-deepseek-r1-distill-qwen-32b
      when:
        - model_name_list is defined
        - "'cpu-deepseek-r1-distill-qwen-32b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy CPU Deepseek R1 Distill Qwen 32b LLM model
      block:
        - name: Delete Ingress resource  Deepseek R1 Distill Qwen 32b from default namespace
          tags: install-cpu-deepseek-r1-distill-qwen-32b
          ignore_errors: yes
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-deepseek-r1-qwen32b-cpu-ingress
            state: absent
        - name: Delete Ingress resource  Deepseek R1 Distill Qwen 32b from auth-apisix namespace
          tags: install-cpu-deepseek-r1-distill-qwen-32b
          ignore_errors: yes
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-deepseek-r1-qwen32b-cpu-ingress
            state: absent
        - name: Deploy CPU based LLM model Deepseek R1 Distill Qwen 32b Installation
          ansible.builtin.command: >-
            helm upgrade --install vllm-deepseek-r1-qwen32b-cpu "{{ remote_helm_charts_base }}/vllm"
            --values "{{ xeon_values_file }}"
            --set LLM_MODEL_ID="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            {% if cpu_playbook == 'true' %}
            --set cpu_balloon_annotation="vllm-balloon"
            --set podLabels.name="vllm"
            --set cpu="{{ optimal_balloon_config.workload_cpus | default(8) }}"
            --set memory="{{ optimal_memory_gb | default(8) }}Gi"
            --set tensor_parallel_size={{ model_paralletensor_parallel_size | default(1) }}
            --set pipeline_parallel_size={{ pipeline_parallel_size | default(1) }}
            {% endif %}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if kubernetes_platform == 'openshift' %}
              --set route.enabled=true
              --set route.host={{ secret_name }}
              --set route.tls.termination=edge
              --set route.tls.insecureEdgeTerminationPolicy=Redirect
              --set ingress.enabled=false
            {% elif ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              --set route.enabled=false
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
              {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment_cpu_deepseek_qwen32b
          failed_when: helm_upgrade_install_model_deployment_cpu_deepseek_qwen32b.rc != 0
        - name: Register Xeon/CPU Deepseek R1 Distill Qwen 32b model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            reg_litellm_model: "openai/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-deepseek-r1-qwen32b-cpu-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-cpu-deepseek-r1-distill-qwen-32b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-cpu-deepseek-r1-distill-qwen-32b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-cpu-deepseek-r1-distill-qwen-32b
      when:
        - model_name_list is defined
        - cpu_playbook == 'true'
        - install_true == 'true'
        - "'cpu-deepseek-r1-distill-qwen-32b' in (model_name_list | regex_replace(',', ' ') | split())"
    
    - name: Check if CPU Deepseek R1 Distill Llama 8B is installed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-deepseek-r1-llama-8b-cpu --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-cpu-deepseek-r1-distill-llama8b
    - name: Uninstall Deepseek R1 Distill Llama 8B
      ansible.builtin.command:
        cmd: "helm uninstall vllm-deepseek-r1-llama-8b-cpu"
      run_once: true
      tags: uninstall-cpu-deepseek-r1-distill-llama8b
      when:
        - "'cpu-deepseek-r1-distill-llama8b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy CPU Deepseek R1 Distill Llama 8B LLM model
      block:
        - name: Delete Ingress resource Deepseek R1 Distill Llama 8B from default namespace
          tags: install-cpu-deepseek-r1-distill-llama8b
          ignore_errors: yes
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-deepseek-r1-llama-8b-cpu-ingress
            state: absent
        - name: Delete Ingress resource Deepseek R1 Distill Llama 8B from auth-apisix namespace
          tags: install-cpu-deepseek-r1-distill-llama8b
          ignore_errors: yes
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-deepseek-r1-llama-8b-cpu-ingress
            state: absent
        - name: Deploy CPU based LLM model Deepseek R1 Distill Llama 8B Installation
          ansible.builtin.command: >-
            helm upgrade --install vllm-deepseek-r1-llama-8b-cpu "{{ remote_helm_charts_base }}/vllm"
            --values "{{ xeon_values_file }}"
            --set LLM_MODEL_ID="deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            {% if cpu_playbook == 'true' %}
            --set cpu_balloon_annotation="vllm-balloon"
            --set podLabels.name="vllm"
            --set cpu="{{ optimal_balloon_config.workload_cpus | default(8) }}"
            --set memory="{{ optimal_memory_gb | default(8) }}Gi"
            --set tensor_parallel_size={{ tensor_parallel_size | default(1) }}
            --set pipeline_parallel_size={{ pipeline_parallel_size | default(1) }}
            {% endif %}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if kubernetes_platform == 'openshift' %}
              --set route.enabled=true
              --set route.host={{ secret_name }}
              --set route.tls.termination=edge
              --set route.tls.insecureEdgeTerminationPolicy=Redirect
              --set ingress.enabled=false
            {% elif ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              --set route.enabled=false
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
              {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment_cpu_deepseek_llama_8b
          failed_when: helm_upgrade_install_model_deployment_cpu_deepseek_llama_8b.rc != 0
        - name: Register Xeon/CPU Deepseek R1 Distill Llama 8B model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
            reg_litellm_model: "openai/deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-deepseek-r1-llama-8b-cpu-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-cpu-deepseek-r1-distill-llama8b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-cpu-deepseek-r1-distill-llama8b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-cpu-deepseek-r1-distill-llama8b
      when:
        - cpu_playbook == 'true'
        - install_true == 'true'
        - "'cpu-deepseek-r1-distill-llama8b' in (model_name_list | regex_replace(',', ' ') | split())"




    - name: Check if CPU Llama-3.2-3B is installed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-llama-3-2-3b-cpu --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-cpu-llama-3-2-3b
    - name: Uninstall Llama-3.2-3B
      ansible.builtin.command:
        cmd: "helm uninstall vllm-llama-3-2-3b-cpu"
      run_once: true
      tags: uninstall-cpu-llama-3-2-3b
      when:
        - model_name_list is defined
        - "'cpu-llama-3-2-3b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy CPU Llama-3.2-3B LLM model
      block:
        - name: Delete Ingress resource Llama-3.2-3B from default namespace
          tags: install-cpu-llama-3-2-3b
          ignore_errors: yes
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-llama-3-2-3b-cpu-ingress
            state: absent
        - name: Delete Ingress resource Llama-3.2-3B from auth-apisix namespace
          tags: install-cpu-llama-3-2-3b
          ignore_errors: yes
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-llama-3-2-3b-cpu-ingress
            state: absent
        - name: Deploy CPU based LLM model Llama-3.2-3B Installation
          ansible.builtin.command: >-
            helm upgrade --install vllm-llama-3-2-3b-cpu "{{ remote_helm_charts_base }}/vllm"
            --values "{{ xeon_values_file }}"
            --set LLM_MODEL_ID="meta-llama/Llama-3.2-3B-Instruct"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            {% if cpu_playbook == 'true' %}
            --set cpu_balloon_annotation="vllm-balloon"
            --set podLabels.name="vllm"
            --set cpu="{{ optimal_balloon_config.workload_cpus | default(8) }}"
            --set memory="{{ optimal_memory_gb | default(8) }}Gi"
            --set tensor_parallel_size={{ tensor_parallel_size | default(1) }}
            --set pipeline_parallel_size={{ pipeline_parallel_size | default(1) }}
            {% endif %}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if kubernetes_platform == 'openshift' %}
              --set route.enabled=true
              --set route.host={{ secret_name }}
              --set route.tls.termination=edge
              --set route.tls.insecureEdgeTerminationPolicy=Redirect
              --set ingress.enabled=false
            {% elif ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              --set route.enabled=false
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
              {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment_cpu_llama3_2_3b_cpu
          failed_when: helm_upgrade_install_model_deployment_cpu_llama3_2_3b_cpu.rc != 0
        - name: Register Xeon/CPU Llama-3.2-3B model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "meta-llama/Llama-3.2-3B-Instruct"
            reg_litellm_model: "openai/meta-llama/Llama-3.2-3B-Instruct"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-llama-3-2-3b-cpu-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-cpu-llama-3-2-3b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-cpu-llama-3-2-3b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-cpu-llama-3-2-3b
      when:
        - model_name_list is defined
        - cpu_playbook == 'true'
        - install_true == 'true'
        - "'cpu-llama-3-2-3b' in (model_name_list | regex_replace(',', ' ') | split())"








    - name: Check if CPU Qwen-3-1-7B is installed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-qwen-3-1-7b-cpu --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-cpu-qwen3-1-7b
    - name: Uninstall Qwen-3-1-7B
      ansible.builtin.command:
        cmd: "helm uninstall vllm-qwen-3-1-7b-cpu"
      run_once: true
      tags: uninstall-cpu-qwen3-1-7b
      when:
        - model_name_list is defined
        - "'cpu-qwen3-1-7b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy CPU  Qwen-3-1-7B LLM model
      block:
        - name: Delete Ingress resource Qwen-3-1-7B from default namespace
          tags: install-cpu-qwen3-1-7b
          ignore_errors: yes
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-qwen-3-1-7b-cpu-ingress
            state: absent
        - name: Delete Ingress resource  Qwen-3-1-7B from auth-apisix namespace
          tags: install-cpu-qwen3-1-7b
          ignore_errors: yes
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-qwen-3-1-7b-cpu-ingress
            state: absent
        - name: Deploy CPU based LLM model Qwen-3-1-7B Installation
          ansible.builtin.command: >-
            helm upgrade --install vllm-qwen-3-1-7b-cpu "{{ remote_helm_charts_base }}/vllm"
            --values "{{ xeon_values_file }}"
            --set LLM_MODEL_ID="Qwen/Qwen3-1.7B"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            {% if cpu_playbook == 'true' %}
            --set cpu_balloon_annotation="vllm-balloon"
            --set podLabels.name="vllm"
            --set cpu="{{ optimal_balloon_config.workload_cpus | default(8) }}"
            --set memory="{{ optimal_memory_gb | default(8) }}Gi"
            --set tensor_parallel_size={{ tensor_parallel_size | default(1) }}
            --set pipeline_parallel_size={{ pipeline_parallel_size | default(1) }}
            {% endif %}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if kubernetes_platform == 'openshift' %}
              --set route.enabled=true
              --set route.host={{ secret_name }}
              --set route.tls.termination=edge
              --set route.tls.insecureEdgeTerminationPolicy=Redirect
              --set ingress.enabled=false
            {% elif ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              --set route.enabled=false
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
              {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment_cpu_llama3_2_3b_cpu
          failed_when: helm_upgrade_install_model_deployment_cpu_llama3_2_3b_cpu.rc != 0
        - name: Register Xeon/CPU Qwen-3-1-7B model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "Qwen/Qwen3-1.7B"
            reg_litellm_model: "openai/Qwen/Qwen3-1.7B"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-qwen-3-1-7b-cpu-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-cpu-qwen3-1-7b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-cpu-qwen3-1-7b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-cpu-qwen3-1-7b
      when:
        - model_name_list is defined
        - cpu_playbook == 'true'
        - install_true == 'true'
        - "'cpu-qwen3-1-7b' in (model_name_list | regex_replace(',', ' ') | split())"



    - name: Check if CPU Qwen-3-4B is installed
      ansible.builtin.command:
        cmd: "helm list --filter vllm-qwen-3-4b-cpu --short"
      register: helm_release_installed
      ignore_errors: true
      run_once: true
      tags: uninstall-cpu-qwen3-4b
    - name: Uninstall Qwen-3-4B
      ansible.builtin.command:
        cmd: "helm uninstall vllm-qwen-3-4b-cpu"
      run_once: true
      tags: uninstall-cpu-qwen3-4b
      when:
        - model_name_list is defined
        - "'cpu-qwen3-4b' in (model_name_list | regex_replace(',', ' ') | split())"
        - uninstall_true == 'true'
        - helm_release_installed.stdout != ""

    - name: Deploy CPU  Qwen-3-4B LLM model
      block:
        - name: Delete Ingress resource Qwen-3-4B from default namespace
          tags: install-cpu-qwen3-4b
          ignore_errors: yes
          kubernetes.core.k8s:
            kind: Ingress
            namespace: default
            name: vllm-qwen-3-4b-cpu-ingress
            state: absent
        - name: Delete Ingress resource   Qwen-3-4B from auth-apisix namespace
          tags: install-cpu-qwen3-4b
          ignore_errors: yes
          kubernetes.core.k8s:
            kind: Ingress
            namespace: auth-apisix
            name: vllm-cpu-qwen-3-4b-cpu-ingress
            state: absent
        - name: Deploy CPU based LLM model  Qwen-3-4B Installation
          ansible.builtin.command: >-
            helm upgrade --install vllm-qwen-3-4b-cpu "{{ remote_helm_charts_base }}/vllm"
            --values "{{ xeon_values_file }}"
            --set LLM_MODEL_ID="Qwen/Qwen3-4B-Instruct-2507"
            --set global.monitoring="{{ vllm_metrics_enabled }}"
            --set svcmonitor.enabled="{{ vllm_metrics_enabled }}"
            --set global.HUGGINGFACEHUB_API_TOKEN={{ hugging_face_token }}
            {% if cpu_playbook == 'true' %}
            --set cpu_balloon_annotation="vllm-balloon"
            --set podLabels.name="vllm"
            --set cpu="{{ optimal_balloon_config.workload_cpus | default(8) }}"
            --set memory="{{ optimal_memory_gb | default(8) }}Gi"
            --set tensor_parallel_size={{ tensor_parallel_size | default(1) }}
            --set pipeline_parallel_size={{ pipeline_parallel_size | default(1) }}
            {% endif %}
            {% if apisix_enabled %}
              --set apisix.enabled={{ apisix_enabled }}
              --set platform={{ kubernetes_platform }}
            {% endif %}
            {% if kubernetes_platform == 'openshift' %}
              --set route.enabled=true
              --set route.host={{ secret_name }}
              --set route.tls.termination=edge
              --set route.tls.insecureEdgeTerminationPolicy=Redirect
              --set ingress.enabled=false
            {% elif ingress_enabled %}
              --set ingress.enabled={{ ingress_enabled }}
              --set ingress.host={{ secret_name }}
              --set ingress.secretname={{ secret_name }}
              --set route.enabled=false
              {% if kubernetes_platform == "eks" %}
              --set aws_certificate_arn={{ aws_certificate_arn | default('') }}
              {% endif %}
            {% endif %}
            {% if deploy_keycloak == 'yes' and apisix_enabled %}
              --set oidc.client_id={{ keycloak_client_id | default('') }}
              --set oidc.client_secret={{ client_secret | default('') }}
            {% endif %}
            {{ helm_proxy_args | default('') }}
            --force
          register: helm_upgrade_install_model_deployment_cpu_llama3_2_3b_cpu
          failed_when: helm_upgrade_install_model_deployment_cpu_llama3_2_3b_cpu.rc != 0
        - name: Register Xeon/CPU  Qwen-3-4B model
          import_tasks: register-model-genai-gateway.yml
          vars:
            reg_model_name: "Qwen/Qwen3-4B-Instruct-2507"
            reg_litellm_model: "openai/Qwen/Qwen3-4B-Instruct-2507"
            reg_custom_llm_provider: "openai"
            reg_api_base: "http://vllm-qwen-3-4b-cpu-service.default/v1"
            reg_input_cost_per_token: 0.001
            reg_output_cost_per_token: 0.002
          tags:
            - install-cpu-qwen3-4b
            - install-genai-gateway
          run_once: true
          when:
            - "'install-cpu-qwen3-4b' in ansible_run_tags"
            - "'install-genai-gateway' in ansible_run_tags"
      run_once: true
      tags: install-cpu-qwen3-4b
      when:
        - model_name_list is defined
        - cpu_playbook == 'true'
        - install_true == 'true'
        - "'cpu-qwen3-4b' in (model_name_list | regex_replace(',', ' ') | split())"



    - name: List of Models to be Installed
      tags: always
      run_once: true
      debug:
        var: model_name_list

    - name: List the Installed Inference Models
      ansible.builtin.shell:
        cmd: "helm list --short | grep 'vllm-'"
      register: inference_models
      when: list_model_true == 'true'
    - name: Print Installed Models in Comma Separated Format
      ansible.builtin.debug:
        msg: "Installed Models: {{ inference_models.stdout_lines | join(', ') }}"
      when: inference_models.stdout_lines is defined and inference_models.stdout_lines | length > 0

    - name: Clean up remote dependencies directory
      tags: always
      ansible.builtin.file:
        path: "{{ remote_helm_charts_base }}"
        state: absent