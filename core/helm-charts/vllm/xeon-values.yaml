# Copyright (C) 2025-2026 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# Xeon CPU-optimized override values for vLLM deployments
# This file contains lean, CPU-specific overrides for Intel Xeon processors
# Base values are inherited from values.yaml

# CPU deployment - no accelerator device
accelDevice: ""

# CPU Balloon configuration for NRI resource policy
cpu_balloon_annotation: ""


resources:
  requests:
    cpu: "{{ .Values.cpu }}"
    memory: "{{ .Values.memory }}"

# CPU-specific configurations
block_size: 128
max_num_seqs: 256
max_seq_len_to_capture: 2048
d_type: "bfloat16"
max_model_len: 8192

# vLLM CPU image configuration
image:
  repository: public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo
  tag: "v0.10.2"
  pullPolicy: IfNotPresent

# Node affinity for Xeon inference nodes
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: ei-inference-eligible
          operator: In
          values: ["true"]

# vLLM CPU environment variables
VLLM_CPU_SGL_KERNEL: "1"
VLLM_NO_USAGE_STATS: 1
DO_NOT_TRACK: 1

LLM_MODEL_ID: ""


modelConfigs:
  "meta-llama/Llama-3.1-8B-Instruct":
    configMapValues:
      VLLM_CPU_KVCACHE_SPACE: "40"
      VLLM_RPC_TIMEOUT: "100000"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
      VLLM_ENGINE_ITERATION_TIMEOUT_S: "120"
      VLLM_CPU_NUM_OF_RESERVED_CPU: "0"
      HF_HUB_DISABLE_XET: "1"
      VLLM_CPU_SGL_KERNEL: "1"
    extraCmdArgs:
      [
        "--block-size",
        "128",
        "--dtype",
        "bfloat16",
        "--max-model-len",
        "8192",
        "--distributed_executor_backend",
        "mp",
        "--enable_chunked_prefill",
        "--enforce-eager",
        "--max-num-batched-tokens",
        "2048",
        "--max-num-seqs",
        "256",
        "--disable-log-requests",
        "--enable-auto-tool-choice",
        "--tool-call-parser", "llama3_json",
      ]
    tensor_parallel_size: "{{ .Values.tensor_parallel_size }}"
    pipeline_parallel_size: "{{ .Values.pipeline_parallel_size }}"

  "meta-llama/Llama-3.2-3B-Instruct":
    configMapValues:
      VLLM_CPU_KVCACHE_SPACE: "40"
      VLLM_RPC_TIMEOUT: "100000"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
      VLLM_ENGINE_ITERATION_TIMEOUT_S: "120"
      VLLM_CPU_NUM_OF_RESERVED_CPU: "0"
      VLLM_CPU_SGL_KERNEL: "1"
      HF_HUB_DISABLE_XET: "1"
    extraCmdArgs:
      [
        "--block-size",
      "128",
      "--dtype",
      "bfloat16",
      "--max-model-len",
      "8192",
      "--distributed_executor_backend",
      "mp",
      "--enable_chunked_prefill",
      "--enforce-eager",
      "--max-num-batched-tokens",
      "2048",
      "--max-num-seqs",
      "256",
       "--enable-auto-tool-choice",
       "--tool-call-parser", "llama3_json",
      ]
    tensor_parallel_size: "{{ .Values.tensor_parallel_size }}"
    pipeline_parallel_size: "{{ .Values.pipeline_parallel_size }}"

  "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B":
    configMapValues:
      VLLM_CPU_KVCACHE_SPACE: "40"
      VLLM_RPC_TIMEOUT: "100000"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
      VLLM_ENGINE_ITERATION_TIMEOUT_S: "120"
      VLLM_CPU_NUM_OF_RESERVED_CPU: "0"
      VLLM_CPU_SGL_KERNEL: "1"
      HF_HUB_DISABLE_XET: "1"
    extraCmdArgs:
      [
        "--block-size",
        "128",
        "--dtype",
        "bfloat16",
        "--max-model-len",
        "8192",
        "--distributed_executor_backend",
        "mp",
        "--enable_chunked_prefill",
        "--enforce-eager",
        "--max-num-batched-tokens",
        "2048",
        "--max-num-seqs",
        "256",
        "--disable-log-requests",        
      ]
    tensor_parallel_size: "{{ .Values.tensor_parallel_size }}"
    pipeline_parallel_size: "{{ .Values.pipeline_parallel_size }}"

  "deepseek-ai/DeepSeek-R1-Distill-Llama-8B":
    configMapValues:
      VLLM_CPU_KVCACHE_SPACE: "40"
      VLLM_RPC_TIMEOUT: "100000"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
      VLLM_ENGINE_ITERATION_TIMEOUT_S: "120"
      VLLM_CPU_NUM_OF_RESERVED_CPU: "0"
      VLLM_CPU_SGL_KERNEL: "1"
      HF_HUB_DISABLE_XET: "1"
    extraCmdArgs:
      [
        "--block-size",
        "128",
        "--dtype",
        "bfloat16",
        "--max-model-len",
        "8192",
        "--distributed_executor_backend",
        "mp",
        "--enable_chunked_prefill",
        "--enforce-eager",
        "--max-num-batched-tokens",
        "2048",
        "--max-num-seqs",
        "256",
        "--disable-log-requests",       
      ]
    tensor_parallel_size: "{{ .Values.tensor_parallel_size }}"
    pipeline_parallel_size: "{{ .Values.pipeline_parallel_size }}"

  "Qwen/Qwen3-1.7B":
    configMapValues:
      VLLM_CPU_KVCACHE_SPACE: "40"
      VLLM_RPC_TIMEOUT: "100000"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
      VLLM_ENGINE_ITERATION_TIMEOUT_S: "120"
      VLLM_CPU_NUM_OF_RESERVED_CPU: "0"
      VLLM_CPU_SGL_KERNEL: "1"
      HF_HUB_DISABLE_XET: "1"
    extraCmdArgs:
      [
        "--block-size",
      "128",
      "--dtype",
      "bfloat16",
      "--max-model-len",
      "8192",
      "--distributed_executor_backend",
      "mp",
      "--enable_chunked_prefill",
      "--enforce-eager",
      "--max-num-batched-tokens",
      "2048",
      "--max-num-seqs",
      "256",
       "--enable-auto-tool-choice",
       "--tool-call-parser", "hermes",
      ]
    tensor_parallel_size: "{{ .Values.tensor_parallel_size }}"
    pipeline_parallel_size: "{{ .Values.pipeline_parallel_size }}"

  "Qwen/Qwen3-4B-Instruct-2507":
    configMapValues:
      VLLM_CPU_KVCACHE_SPACE: "40"
      VLLM_RPC_TIMEOUT: "100000"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
      VLLM_ENGINE_ITERATION_TIMEOUT_S: "120"
      VLLM_CPU_NUM_OF_RESERVED_CPU: "0"
      VLLM_CPU_SGL_KERNEL: "1"
      HF_HUB_DISABLE_XET: "1"
    extraCmdArgs:
      [
        "--block-size",
      "128",
      "--dtype",
      "bfloat16",
      "--max-model-len",
      "8192",
      "--distributed_executor_backend",
      "mp",
      "--enable_chunked_prefill",
      "--enforce-eager",
      "--max-num-batched-tokens",
      "2048",
      "--max-num-seqs",
      "256",
       "--enable-auto-tool-choice",
       "--tool-call-parser", "hermes",
      ]
    tensor_parallel_size: "{{ .Values.tensor_parallel_size }}"
    pipeline_parallel_size: "{{ .Values.pipeline_parallel_size }}"

defaultModelConfigs:
  configMapValues:
    VLLM_CPU_KVCACHE_SPACE: "40"
    VLLM_RPC_TIMEOUT: "100000"
    VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
    VLLM_ENGINE_ITERATION_TIMEOUT_S: "120"
    VLLM_CPU_NUM_OF_RESERVED_CPU: "0"
    VLLM_CPU_SGL_KERNEL: "1"
    HF_HUB_DISABLE_XET: "1"
  extraCmdArgs:
    [
      "--block-size",
      "128",
      "--dtype",
      "bfloat16",
      "--max-model-len",
      "8192",
      "--distributed_executor_backend",
      "mp",
      "--enable_chunked_prefill",
      "--enforce-eager",
      "--max-num-batched-tokens",
      "2048",
      "--max-num-seqs",
      "256",
    ]
  tensor_parallel_size: "{{ .Values.tensor_parallel_size }}"
  pipeline_parallel_size: "{{ .Values.pipeline_parallel_size }}"
