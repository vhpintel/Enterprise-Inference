# Copyright (C) 2024-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# Xeon CPU-optimized override values for vLLM deployments
# This file contains lean, CPU-specific overrides for Intel Xeon processors
# Base values are inherited from values.yaml

# CPU deployment - no accelerator device
accelDevice: ""

# CPU Balloon configuration for NRI resource policy
cpu_balloon_annotation: ""


resources:
  requests:
    cpu: "{{ .Values.cpu }}"
    memory: "{{ .Values.memory }}"


# CPU-specific configurations
block_size: 128
max_num_seqs: 256
max_seq_len_to_capture: 2048
d_type: "bfloat16"
max_model_len: 8192

# vLLM CPU image configuration
image:
  repository: public.ecr.aws/q9t5s3a7/vllm-cpu-release-repo
  tag: "v0.10.2"
  pullPolicy: IfNotPresent

# Node affinity for Xeon inference nodes
affinity:
  nodeAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
            - key: node-role.kubernetes.io/inference-xeon
              operator: In
              values:
                - "true"
      - weight: 90
        preference:
          matchExpressions:
            - key: node-role.kubernetes.io/inference-cpu
              operator: In
              values:
                - "true"

# vLLM CPU environment variables
VLLM_CPU_SGL_KERNEL: "1"
VLLM_NO_USAGE_STATS: 1
DO_NOT_TRACK: 1

LLM_MODEL_ID: ""


modelConfigs:
  "meta-llama/Llama-3.1-8B-Instruct":
    configMapValues:
      VLLM_CPU_KVCACHE_SPACE: "40"
      VLLM_RPC_TIMEOUT: "100000"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
      VLLM_ENGINE_ITERATION_TIMEOUT_S: "120"
      VLLM_CPU_NUM_OF_RESERVED_CPU: "0"
      VLLM_CPU_SGL_KERNEL: "1"
    extraCmdArgs:
      [
        "--block-size",
        "128",
        "--dtype",
        "bfloat16",
        "--max-model-len",
        "8192",
        "--distributed_executor_backend",
        "mp",
        "--enable_chunked_prefill",
        "--enforce-eager",
        "--max-num-batched-tokens",
        "2048",
        "--max-num-seqs",
        "256",
        "--disable-log-requests",
      ]
    tensor_parallel_size: "{{ .Values.tensor_parallel_size }}"
    pipeline_parallel_size: "{{ .Values.pipeline_parallel_size }}"

  "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B":
    configMapValues:
      VLLM_CPU_KVCACHE_SPACE: "40"
      VLLM_RPC_TIMEOUT: "100000"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
      VLLM_ENGINE_ITERATION_TIMEOUT_S: "120"
      VLLM_CPU_NUM_OF_RESERVED_CPU: "0"
      VLLM_CPU_SGL_KERNEL: "1"
    extraCmdArgs:
      [
        "--block-size",
        "128",
        "--dtype",
        "bfloat16",
        "--max-model-len",
        "8192",
        "--distributed_executor_backend",
        "mp",
        "--enable_chunked_prefill",
        "--enforce-eager",
        "--max-num-batched-tokens",
        "2048",
        "--max-num-seqs",
        "256",
        "--disable-log-requests",
      ]
    tensor_parallel_size: "{{ .Values.tensor_parallel_size }}"
    pipeline_parallel_size: "{{ .Values.pipeline_parallel_size }}"

  "deepseek-ai/DeepSeek-R1-Distill-Llama-8B":
    configMapValues:
      VLLM_CPU_KVCACHE_SPACE: "40"
      VLLM_RPC_TIMEOUT: "100000"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
      VLLM_ENGINE_ITERATION_TIMEOUT_S: "120"
      VLLM_CPU_NUM_OF_RESERVED_CPU: "0"
      VLLM_CPU_SGL_KERNEL: "1"
    extraCmdArgs:
      [
        "--block-size",
        "128",
        "--dtype",
        "bfloat16",
        "--max-model-len",
        "8192",
        "--distributed_executor_backend",
        "mp",
        "--enable_chunked_prefill",
        "--enforce-eager",
        "--max-num-batched-tokens",
        "2048",
        "--max-num-seqs",
        "256",
        "--disable-log-requests",
      ]
    tensor_parallel_size: "{{ .Values.tensor_parallel_size }}"
    pipeline_parallel_size: "{{ .Values.pipeline_parallel_size }}"

defaultModelConfigs:
  configMapValues:
    VLLM_CPU_KVCACHE_SPACE: "40"
    VLLM_RPC_TIMEOUT: "100000"
    VLLM_ALLOW_LONG_MAX_MODEL_LEN: "1"
    VLLM_ENGINE_ITERATION_TIMEOUT_S: "120"
    VLLM_CPU_NUM_OF_RESERVED_CPU: "0"
    VLLM_CPU_SGL_KERNEL: "1"
  extraCmdArgs:
    [
      "--block-size",
      "128",
      "--dtype",
      "bfloat16",
      "--max-model-len",
      "8192",
      "--distributed_executor_backend",
      "mp",
      "--enable_chunked_prefill",
      "--enforce-eager",
      "--max-num-batched-tokens",
      "2048",
      "--max-num-seqs",
      "256",
    ]
  tensor_parallel_size: "{{ .Values.tensor_parallel_size }}"
  pipeline_parallel_size: "{{ .Values.pipeline_parallel_size }}"