# Copyright (C) 2024-2025 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

# User-configurable parameters (can be set via --set during helm install)
accelDevice: "gaudi"

block_size: 128 # Default KV cache block size
max_num_seqs: 256 # Default maximum number of sequences
max_seq_len_to_capture: 2048 # Default maximum sequence length
d_type: "bfloat16"
max_model_len: 5120

image:
  repository: opea/vllm-gaudi
  tag: "1.22.0"
  pullPolicy: IfNotPresent

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: role
          operator: In
          values: ["inference"]
      - matchExpressions:
        - key: role
          operator: In
          values: ["inference-gaudi"]

runtime: "habana"
HABANA_VISIBLE_DEVICES: "all"
VLLM_NO_USAGE_STATS: 1
DO_NOT_TRACK: 1

LLM_MODEL_ID: ""

modelConfigs:
  # Qwen/Qwen2.5-7B-Instruct
  "Qwen/Qwen2.5-7B-Instruct":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 107328
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.3
      VLLM_GRAPH_RESERVED_MEM: 0.16
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,1024"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","416","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--disable-log-requests", "--tool-call-parser","hermes","--chat-template","/workspace/vllm/examples/tool_chat_template_hermes.jinja", "--enable-auto-tool-choice"]
    tensor_parallel_size: "1"
  
  # Qwen/Qwen2.5-14B-Instruct
  "Qwen/Qwen2.5-14B-Instruct":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 57792
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.6
      VLLM_GRAPH_RESERVED_MEM: 0.07
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,1024"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.98","--max-num-seqs","224","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--disable-log-requests", "--tool-call-parser","hermes","--chat-template","/workspace/vllm/examples/tool_chat_template_hermes.jinja", "--enable-auto-tool-choice"]
    tensor_parallel_size: "1"

  # Qwen/Qwen2.5-32B-Instruct
  "Qwen/Qwen2.5-32B-Instruct":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 16512
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.9
      VLLM_GRAPH_RESERVED_MEM: 0.13
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,1024"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.98","--max-num-seqs","64","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--disable-log-requests","--tool-call-parser","hermes","--chat-template","/workspace/vllm/examples/tool_chat_template_hermes.jinja", "--enable-auto-tool-choice" ]
    tensor_parallel_size: "1"

  # meta-llama/Llama-3.1-8B-Instruct
  "meta-llama/Llama-3.1-8B-Instruct":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 57792
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.5
      VLLM_GRAPH_RESERVED_MEM: 0.07
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,1024"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","224","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "1"

  # meta-llama/Llama-3.1-70B-Instruct
  "meta-llama/Llama-3.1-70B-Instruct":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 66048
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.4
      VLLM_GRAPH_RESERVED_MEM: 0.1
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,2048"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.98","--max-num-seqs","256","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "4"

  # meta-llama/Llama-3.2-3B-Instruct
  "meta-llama/Llama-3.2-3B-Instruct":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 66048
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.5
      VLLM_GRAPH_RESERVED_MEM: 0.09
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,1024"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","256","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.2_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "1"

  # meta-llama/Llama-3.3-70B-Instruct
  "meta-llama/Llama-3.3-70B-Instruct":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 66048
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.4
      VLLM_GRAPH_RESERVED_MEM: 0.1
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,2048"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.98","--max-num-seqs","256","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.2_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "4"

  # meta-llama/Llama-3.1-405B-Instruct
  "meta-llama/Llama-3.1-405B-Instruct":
    configMapValues:
      PT_HPU_LAZY_MODE: "1"
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      EXPERIMENTAL_WEIGHT_SHARING: "0"
      VLLM_DELAYED_SAMPLING: "true"
      VLLM_SKIP_WARMUP: "false"
      VLLM_GRAPH_RESERVED_MEM: "0.19"
      VLLM_GRAPH_PROMPT_RATIO: "0.8"
      VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
      VLLM_PROMPT_SEQ_BUCKET_STEP: "256"
      VLLM_PROMPT_SEQ_BUCKET_MAX: "33024"
      VLLM_DECODE_BLOCK_BUCKET_MAX: "8256"
      VLLM_DECODE_BS_BUCKET_STEP: "32"
      VLLM_PROMPT_BS_BUCKET_STEP: "32"
      HF_HUB_DISABLE_XET: 1
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,8192"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.96","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "8"

  # mistralai/Mistral-7B-Instruct-v0.2
  "mistralai/Mistral-7B-Instruct-v0.2":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 57792
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.5
      VLLM_GRAPH_RESERVED_MEM: 0.07
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,1024"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","224","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling","--tool-call-parser","mistral","--chat-template","/workspace/vllm/examples/tool_chat_template_mistral.jinja", "--enable-auto-tool-choice"]
    tensor_parallel_size: "1"

  
  # mistralai/Mistral-7B-Instruct-v0.3
  "mistralai/Mistral-7B-Instruct-v0.3":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 57792
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.5
      VLLM_GRAPH_RESERVED_MEM: 0.07
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,1024"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","224","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--tool-call-parser","mistral","--chat-template","/workspace/vllm/examples/tool_chat_template_mistral.jinja", "--enable-auto-tool-choice"]
    tensor_parallel_size: "1"
  
  # mistralai/Mistral-7B-Instruct-v0.1
  "mistralai/Mistral-7B-Instruct-v0.1":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 74304
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.4
      VLLM_GRAPH_RESERVED_MEM: 0.13
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,1024"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.98","--max-num-seqs","288","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling","--tool-call-parser","mistral","--chat-template","/workspace/vllm/examples/tool_chat_template_mistral.jinja", "--enable-auto-tool-choice"]
    tensor_parallel_size: "1"

  # mistralai/Mixtral-8x7B-Instruct-v0.1
  "mistralai/Mixtral-8x7B-Instruct-v0.1":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 74304
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.4
      VLLM_GRAPH_RESERVED_MEM: 0.13
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,2048"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.98","--max-num-seqs","288","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--tool-call-parser","mistral","--chat-template","/workspace/vllm/examples/tool_chat_template_mistral.jinja", "--enable-auto-tool-choice"]
    tensor_parallel_size: "2"

  # mistralai/Mixtral-8x22B-Instruct-v0.1
  "mistralai/Mixtral-8x22B-Instruct-v0.1":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 66048
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.5
      VLLM_GRAPH_RESERVED_MEM: 0.17
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,2048"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.98","--max-num-seqs","256","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling","--tool-call-parser","mistral","--chat-template","/workspace/vllm/examples/tool_chat_template_mistral.jinja", "--enable-auto-tool-choice"]
    tensor_parallel_size: "2"

  # tiiuae/Falcon3-7B-Instruct
  "tiiuae/Falcon3-7B-Instruct":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      OMPI_MCA_btl_vader_single_copy_mechanism: none
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      VLLM_CPU_KVCACHE_SPACE: "40"
      HF_HUB_DISABLE_XET: 1
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,2048"
    extraCmdArgs: ["--block-size", "128", "--dtype", "bfloat16", "--max-model-len","5196",]
    tensor_parallel_size: "1"
  
  # ibm-granite/granite-3.1-8b-instruct
  "ibm-granite/granite-3.1-8b-instruct":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 41280
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.6
      VLLM_GRAPH_RESERVED_MEM: 0.06
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,1024"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","160","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--disable-log-requests","--tool-call-parser","granite","--chat-template","/workspace/vllm/examples/tool_chat_template_granite.jinja", "--enable-auto-tool-choice"]
    tensor_parallel_size: "1"

  # ibm-granite/granite-34b-code-instruct-8k
  "ibm-granite/granite-34b-code-instruct-8k":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: false
      PT_HPU_LAZY_MODE: 1
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 107328
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.3
      VLLM_GRAPH_RESERVED_MEM: 0.2
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,1024"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.98","--max-num-seqs","416","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--disable-log-requests","--tool-call-parser","granite","--chat-template","/workspace/vllm/examples/tool_chat_template_granite.jinja", "--enable-auto-tool-choice"]
    tensor_parallel_size: "1"

  # ibm-granite/granite-3b-code-instruct-128k
  "ibm-granite/granite-3b-code-instruct-128k":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 24768
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.8
      VLLM_GRAPH_RESERVED_MEM: 0.06
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,1024"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","96","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--disable-log-requests","--tool-call-parser","granite","--chat-template","/workspace/vllm/examples/tool_chat_template_granite.jinja", "--enable-auto-tool-choice"]
    tensor_parallel_size: "1"

  # deepseek-ai/DeepSeek-R1-Distill-Llama-8B
  "deepseek-ai/DeepSeek-R1-Distill-Llama-8B":
    configMapValues:
      PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
      EXPERIMENTAL_WEIGHT_SHARING: "0"
      VLLM_SKIP_WARMUP: "false"
      VLLM_GRAPH_RESERVED_MEM: "0.05"
      VLLM_DECODE_BLOCK_BUCKET_STEP: "256"
      VLLM_PROMPT_SEQ_BUCKET_MAX: "33024"
      VLLM_DECODE_BLOCK_BUCKET_MAX: "8256"
      VLLM_ALLOW_LONG_MAX_MODEL_LEN: 1
      HF_HUB_DISABLE_XET: 1
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,1024"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "1"

  # deepseek-ai/DeepSeek-R1-Distill-Llama-70B
  "deepseek-ai/DeepSeek-R1-Distill-Llama-70B":
    configMapValues:
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 66048
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EXPONENTIAL_BUCKETING: false
      VLLM_GRAPH_PROMPT_RATIO: 0.4
      VLLM_GRAPH_RESERVED_MEM: 0.1
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      VLLM_PROMPT_USE_FUSEDSDPA: 1
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,2048"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.98","--max-num-seqs","256","--max-num-prefill-seqs","16","--num_scheduler_steps","1","--use-padding-aware-scheduling", "--tool-call-parser","llama3_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama3.1_json.jinja", "--enable-auto-tool-choice", "--disable-log-requests"]
    tensor_parallel_size: "4"

  # deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
  "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B":
    configMapValues:
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_LAZY_MODE: 1
      VLLM_SKIP_WARMUP: false
      VLLM_GRAPH_RESERVED_MEM: 0.05
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_PROMPT_SEQ_BUCKET_MAX: 4352
      VLLM_DECODE_BLOCK_BUCKET_MAX: 6528
      HF_HUB_DISABLE_XET: 1
      PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,1024"
    extraCmdArgs: ["--block-size","128","--dtype","bfloat16","--max-model-len","33024","--gpu-memory-util","0.99","--max-num-seqs","32","--max-num-prefill-seqs","16","--num_scheduler_steps","16","--use-padding-aware-scheduling", "--disable-log-requests"]
    tensor_parallel_size: "2"
  
  # "meta-llama/Llama-4-Scout-17B-16E-Instruct"
  "meta-llama/Llama-4-Scout-17B-16E-Instruct":
    configMapValues:
      HF_HUB_DISABLE_XET: 1
      EXPERIMENTAL_WEIGHT_SHARING: 0
      PT_HPU_ENABLE_LAZY_COLLECTIVES: true
      PT_HPU_LAZY_MODE: 1
      VLLM_DECODE_BLOCK_BUCKET_MAX: 33024
      VLLM_DECODE_BLOCK_BUCKET_STEP: 256
      VLLM_DECODE_BS_BUCKET_STEP: 32
      VLLM_DELAYED_SAMPLING: true
      VLLM_EP_SIZE: 16
      VLLM_GRAPH_PROMPT_RATIO: 0.3
      VLLM_GRAPH_RESERVED_MEM: 0.17
      VLLM_PROMPT_BS_BUCKET_STEP: 32
      VLLM_PROMPT_SEQ_BUCKET_MAX: 33024
      VLLM_PROMPT_SEQ_BUCKET_STEP: 256
      HF_HUB_DISABLE_XET: 1
      VLLM_SKIP_WARMUP: false
    extraCmdArgs: ["--block-size", "128", "--dtype", "bfloat16", "--max-model-len","33024", "--gpu-memory-util","0.98", "--max-num-seqs","128", "--max-num-prefill-seqs","16", "--num_scheduler_steps","1", "--use-padding-aware-scheduling", "--disable-log-requests", "--trust_remote_code", "--enable-expert-parallel", "--use-v2-block-manager", "--tool-call-parser","llama4_json","--chat-template","/workspace/vllm/examples/tool_chat_template_llama4_json.jinja", "--enable-auto-tool-choice", "--override-generation-config={\"attn_temperature_tuning\": true}" ]
    tensor_parallel_size: "4"

# Default arguments if the model ID is not found
defaultModelConfigs:
  configMapValues:
    EXPERIMENTAL_WEIGHT_SHARING: 0
    LLM_MODEL_ID: ""
    OMPI_MCA_btl_vader_single_copy_mechanism: none
    PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
    VLLM_CPU_KVCACHE_SPACE: "40"
    HF_HUB_DISABLE_XET: 1
    PT_HPU_RECIPE_CACHE_CONFIG: "/data/recipe_cache,false,1024"
  extraCmdArgs: ["--block-size", "128", "--dtype", "bfloat16", "--max-model-len","5196"]
  tensor_parallel_size: "1"